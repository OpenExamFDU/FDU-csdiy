# 机器学习系统

### Foundations

* [微软亚洲研究院 - 人工智能系统](https://github.com/microsoft/AI-System)
* [CMU 10-414 Deep Learning Systems](https://dlsyscourse.org/lectures/)
* [刘铁岩 - 分布式机器学习](https://book.douban.com/subject/30360968/)

### Seminars & Collections

* [Stanford MLSys Seminar](https://mlsys.stanford.edu/)
* [Machine Learning System Resources](https://www.bodunhu.com/blog/posts/machine-learning-system-resources/)
* [Building Blocks for AI Systems](https://github.com/HazyResearch/aisys-building-blocks)

### MLSys & ML Compilation

* [CMU 15-442 Machine Learning Systems](https://mlsyscourse.org/)
* [UCB CS294 Machine Learning Systems](https://ucbrise.github.io/cs294-ai-sys-sp22/)
* [CMU Machine Learning Compilation](https://mlc.ai/)
* [UW CS599 ML for ML Systems](https://courses.cs.washington.edu/courses/cse599m/23sp/)

### Hardwares

* [Stanford CS217 Hardware Accelerators for Machine Learning](https://cs217.stanford.edu/)
* [Cornell ECE5545 Machine Learning Hardware and Systems](https://abdelfattah-class.github.io/ece5545/sp23)
* [UCB EE290 Hardware for Machine Learning](https://www.bilibili.com/video/BV1hi4y1A7BC)
* [MIT 6.5930 Hardware Architecture for Deep Learning](http://csg.csail.mit.edu/6.5930/info.html)

### Efficient Deep Learning

* [MIT 6.5940 TinyML and Efficient Deep Learning Computing](https://hanlab.mit.edu/courses/2023-fall-65940)

### LLM Systems

* [UCB CS294 Machine Learning Systems (LLM Edition)](https://learning-systems.notion.site/AI-Systems-LLM-Edition-294-162-Fall-2023-661887583bd340fa851e6a8da8e29abb)
* [CMU 11-868 Large Language Model Systems](https://llmsystem.github.io/llmsystem2024spring/)
* [UMich EECS598 Systems for Generative AI](https://github.com/mosharaf/eecs598/tree/w24-genai)
* [UIUC CS598 AI Efficiency: Systems & Algorithms](https://minjiazhang.github.io/courses/24sp-schedule.html)

### Blogs & Tutorials

* [A Full Hardware Guide to Deep Learning](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/)
* [What Every Developer Should Know About GPU Computing](https://blog.codingconfessions.com/p/gpu-computing)
* [Trends in Machine Learning Hardware](https://epochai.org/blog/trends-in-machine-learning-hardware#computational-price-performance)
* [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed)
* [Exploring the Latency/Throughput & Cost Space for LLM Inference](https://www.youtube.com/watch?v=mYRqvB1\_gRk)
* [Towards 100x Speedup: Full Stack Transformer Inference Optimization](https://yaofu.notion.site/Towards-100x-Speedup-Full-Stack-Transformer-Inference-Optimization-43124c3688e14cffaf2f1d6cbdf26c6c)
* [Full Stack Transformer Inference Optimization Season 2: Deploying Long-Context Models](https://yaofu.notion.site/Full-Stack-Transformer-Inference-Optimization-Season-2-Deploying-Long-Context-Models-ee25d3a77ba14f73b8ae19147f77d5e2)
* [How to make LLMs go fast](https://vgel.me/posts/faster-inference/)
* [How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/posts/2021-09-25-train-large/)
* [Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
* [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/)
* [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr\_intro.html)
* [Mastering LLM Techniques: Inference Optimization](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization)
* [LLM Inference Series: 1. Introduction](https://medium.com/@plienhar/llm-inference-series-1-introduction-9c78e56ef49d)
* [LLM Inference Series: 2. The two-phase process behind LLMs’ responses](https://medium.com/@plienhar/llm-inference-series-2-the-two-phase-process-behind-llms-responses-1ff1ff021cd5)
* [LLM Inference Series: 3. KV caching explained](https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8)
* [LLM Inference Series: 4. KV caching, a deeper look](https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8)
* [LLM Inference Series: 5. Dissecting model performance](https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f)
* [LLM Inference Performance Engineering: Best Practices](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)
* [Reproducible Performance Metrics for LLM inference](https://www.anyscale.com/blog/reproducible-performance-metrics-for-llm-inference)
* [Everything about Distributed Training and Efficient Finetuning](https://sumanthrh.com/post/distributed-and-efficient-finetuning/)
* [Where do LLMs spend their FLOPS?](https://www.artfintel.com/p/where-do-llms-spend-their-flops)
* [剖析GPT推断中的批处理效应](https://abcdabcd987.com/2023/05/13/transformer-batching/)
* [A100/H100 太贵，何不用 4090？](https://zhuanlan.zhihu.com/p/655402388)

