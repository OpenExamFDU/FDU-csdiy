# 机器学习系统

### Foundations

* [微软亚洲研究院 - 人工智能系统](https://github.com/microsoft/AI-System)
* [CMU 10-414 Deep Learning Systems](https://dlsyscourse.org/lectures/)
* [刘铁岩 - 分布式机器学习](https://book.douban.com/subject/30360968/)

### Seminars & Collections

* [Stanford MLSys Seminar](https://mlsys.stanford.edu/)
* [Machine Learning System Resources](https://www.bodunhu.com/blog/posts/machine-learning-system-resources/)
* [Building Blocks for AI Systems](https://github.com/HazyResearch/aisys-building-blocks)
* [AI 系统开源课程](https://chenzomi12.github.io/index.html)

### MLSys & ML Compilation

* [CMU 15-442 Machine Learning Systems](https://mlsyscourse.org/)
* [UCB CS294 Machine Learning Systems](https://ucbrise.github.io/cs294-ai-sys-sp22/)
* [CMU Machine Learning Compilation](https://mlc.ai/)
* [UW CS599 ML for ML Systems](https://courses.cs.washington.edu/courses/cse599m/23sp/)

### Hardwares

* [Stanford CS217 Hardware Accelerators for Machine Learning](https://cs217.stanford.edu/)
* [Cornell ECE5545 Machine Learning Hardware and Systems](https://abdelfattah-class.github.io/ece5545/sp23)
* [UCB EE290 Hardware for Machine Learning](https://www.bilibili.com/video/BV1hi4y1A7BC)
* [MIT 6.5930 Hardware Architecture for Deep Learning](http://csg.csail.mit.edu/6.5930/info.html)

### Efficient Deep Learning

* [MIT 6.5940 TinyML and Efficient Deep Learning Computing](https://hanlab.mit.edu/courses/2023-fall-65940)

### LLM Systems

* [UCB CS294 Machine Learning Systems (LLM Edition)](https://learning-systems.notion.site/AI-Systems-LLM-Edition-294-162-Fall-2023-661887583bd340fa851e6a8da8e29abb)
* [CMU 11-868 Large Language Model Systems](https://llmsystem.github.io/llmsystem2024spring/)
* [UMich EECS598 Systems for Generative AI](https://github.com/mosharaf/eecs598/tree/w24-genai)
* [UIUC CS598 AI Efficiency: Systems & Algorithms](https://minjiazhang.github.io/courses/24sp-schedule.html)

### Blogs & Tutorials

#### Hardwares

* [Tim Dettmers - A Full Hardware Guide to Deep Learning](https://timdettmers.com/2018/12/16/deep-learning-hardware-guide/)
* [What Every Developer Should Know About GPU Computing](https://blog.codingconfessions.com/p/gpu-computing)
* [Trends in Machine Learning Hardware](https://epochai.org/blog/trends-in-machine-learning-hardware#computational-price-performance)

#### Training

* [Huggingface - The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed)
* [Lilian Weng - How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/posts/2021-09-25-train-large/)
* [Andrej Karpathy - Let's reproduce GPT-2 (124M)](https://www.youtube.com/watch?v=l8pRSuU81PU)
* [Everything about Distributed Training and Efficient Finetuning](https://sumanthrh.com/post/distributed-and-efficient-finetuning/)
* [DeepSpeed: Advancing MoE inference and training to power next-generation AI scale](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)
* [DeepSpeed ZeRO++: A leap in speed for LLM and chat model training with 4X less communication](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)
* [DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-ulysses/README.md)
* [DeepSpeed Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales](https://github.com/microsoft/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md)
* [A100/H100 太贵，何不用 4090？](https://zhuanlan.zhihu.com/p/655402388)

#### Inference & Serving

* [Mistral-AI‬ - Exploring the Latency/Throughput & Cost Space for LLM Inference](https://www.youtube.com/watch?v=mYRqvB1\_gRk)
* [Yao Fu - Full Stack Transformer Inference Optimization Season 1: Towards 100x Speedup](https://yaofu.notion.site/Towards-100x-Speedup-Full-Stack-Transformer-Inference-Optimization-43124c3688e14cffaf2f1d6cbdf26c6c)
* [Yao Fu - Full Stack Transformer Inference Optimization Season 2: Deploying Long-Context Models](https://yaofu.notion.site/Full-Stack-Transformer-Inference-Optimization-Season-2-Deploying-Long-Context-Models-ee25d3a77ba14f73b8ae19147f77d5e2)
* [Lilian Weng - Large Transformer Model Inference Optimization](https://lilianweng.github.io/posts/2023-01-10-inference-optimization/)
* [NVIDIA - Mastering LLM Techniques: Inference Optimization](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization)
* [Databricks - LLM Inference Performance Engineering: Best Practices](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)
* [Anyscale - Reproducible Performance Metrics for LLM inference](https://www.anyscale.com/blog/reproducible-performance-metrics-for-llm-inference)
* [Anyscale - How continuous batching enables 23x throughput in LLM inference while reducing p50 latency](https://www.anyscale.com/blog/continuous-batching-llm-inference)
* [How to make LLMs go fast](https://vgel.me/posts/faster-inference/)
* [Where do LLMs spend their FLOPS?](https://www.artfintel.com/p/where-do-llms-spend-their-flops)
* [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr\_intro.html)
* [LLM Inference Series: 1. Introduction](https://medium.com/@plienhar/llm-inference-series-1-introduction-9c78e56ef49d)
* [LLM Inference Series: 2. The two-phase process behind LLMs’ responses](https://medium.com/@plienhar/llm-inference-series-2-the-two-phase-process-behind-llms-responses-1ff1ff021cd5)
* [LLM Inference Series: 3. KV caching explained](https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8)
* [LLM Inference Series: 4. KV caching, a deeper look](https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8)
* [LLM Inference Series: 5. Dissecting model performance](https://medium.com/@plienhar/llm-inference-series-5-dissecting-model-performance-6144aa93168f)
* [Transformer Inference Arithmetic](https://kipp.ly/transformer-inference-arithmetic/)
* [剖析GPT推断中的批处理效应](https://abcdabcd987.com/2023/05/13/transformer-batching/)
* [LLM部署代价评估](https://zhuanlan.zhihu.com/p/658868628)

